{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38951d2-bd3d-46cb-9446-9683534b444f",
   "metadata": {},
   "source": [
    "# Code to regularly update most current OSM data for 02_Settlement_Data\n",
    "\n",
    "### Run this one!\n",
    "\n",
    "I would suggest running this around once per month to ensure that the API rate load does not become too high. However, I have built in safeguards to work around rate-limiting when it comes to redownloading data. The script should run without any issues.\n",
    "\n",
    "Note: the `population` column often saves data in weird and wonky ways. I do not believe there are any long-term issues with the warnings that are passed by this script, and just want to note that as far as I can tell, they are not errors. Also, some countries do not have population data available for individual settlements. This can cause a warning when creating the shapefiles, but I do not believe that it damages them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448fd965-eca6-4e63-8f99-05ddb615481f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/ceph/standard/Plan-EO_Storage/Capstone-25\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import geopandas\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# read in country metadata\n",
    "print(os.getcwd())\n",
    "\n",
    "metadata = pd.read_excel('../Plan-EO_Country_meta-data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd7193c-5a49-4327-8ca9-2ac0b7eda802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>ISO2</th>\n",
       "      <th>ISO3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AO</td>\n",
       "      <td>AGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>AR</td>\n",
       "      <td>ARG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>AM</td>\n",
       "      <td>ARM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name ISO2 ISO3\n",
       "0  Afghanistan   AF  AFG\n",
       "1      Algeria   DZ  DZA\n",
       "2       Angola   AO  AGO\n",
       "3    Argentina   AR  ARG\n",
       "4      Armenia   AM  ARM"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unused metadata columns and relabel country name to match storage folder format\n",
    "metadata = metadata[['Name', 'ISO2', 'ISO3']]\n",
    "metadata.Name = metadata.Name.str.replace(' ', '_')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6efae7a-dd27-4889-bea7-8cc54d06e30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a special Error class I have created to handle errors when the Overpass API rate-limits us.\n",
    "# It does not need to be edited.\n",
    "\n",
    "class APIRateLimitError(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ea290f-1f59-4897-85f8-5fb51ffa06fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need this convert_to_csv() function in both scripts, it handles converting json to pandas DataFrame that can be saved as CSV\n",
    "\n",
    "def convert_to_csv(settlements_json):\n",
    "    \n",
    "    # initialize empty list\n",
    "    settlements = []\n",
    "    \n",
    "    # for updating: If there are no new settlements to update for this country, return None and exit the function\n",
    "    # see below function for how this is handled\n",
    "    if not settlements_json['elements']:\n",
    "        return None\n",
    "    \n",
    "    # loop through new elements\n",
    "    for element in settlements_json['elements']:\n",
    "        osm_id = element['id']\n",
    "        name = element['tags'].get('name')\n",
    "        place_type = element['tags'].get('place')\n",
    "        lat = element.get('lat')\n",
    "        lon = element.get('lon')\n",
    "        population = element['tags'].get('population')\n",
    "        \n",
    "        # add the extracted data to the settlements list\n",
    "        settlements.append({'osm_id': osm_id, 'name': name, 'place': place_type, 'latitude': lat, 'longitude': lon, 'population': population})\n",
    "    \n",
    "    # convert the settlements list to a DataFrame\n",
    "    df = pd.DataFrame(settlements)\n",
    "    df.columns = ['osm_id', 'name', 'place', 'lat', 'lon', 'population']\n",
    "    \n",
    "    # this return, if any, will be concatenated with the old CSV to create a new, fully updated CSV\n",
    "    return df\n",
    "\n",
    "\n",
    "# this is the function to update the CSVs every month\n",
    "# this allows to variably set days (if it's been longer or shorter) with the \"cushion\" argument\n",
    "# but once we determine the schedule we will code it ourselves (probably with a buffer of a couple days, I'm thinking 5-10 extra)\n",
    "\n",
    "def update_settlements(country_code, csv_path, cushion=5):\n",
    "    \n",
    "    # get time length that we want\n",
    "    \n",
    "    # check when file was last modified, convert to datetime\n",
    "    last_mod = os.path.getmtime(csv_path)\n",
    "    dt_last_mod = datetime.datetime.fromtimestamp(last_mod)\n",
    "    \n",
    "    # this is how far back we'll be searching for edits - default is 5 day overlap between updates, but can be changed\n",
    "    since = dt_last_mod - datetime.timedelta(days=cushion)\n",
    "    \n",
    "    # write Overpass API query (note the 300 second timeout period, and how the \"since\" is incorporated)\n",
    "    query = f\"\"\"\n",
    "    [out:json][timeout:300];\n",
    "        area[\"ISO3166-1\"=\"{country_code}\"]->.country;\n",
    "    (\n",
    "      node[\"place\"](area.country)(newer:\"{since:%Y-%m-%d}T00:00:00Z\");\n",
    "      way[\"place\"](area.country)(newer:\"{since:%Y-%m-%d}T00:00:00Z\");\n",
    "      relation[\"place\"](area.country)(newer:\"{since:%Y-%m-%d}T00:00:00Z\");\n",
    "    );\n",
    "    out center;\n",
    "    \"\"\"\n",
    "    \n",
    "    # this query will return every settlement that has been changed since the last time we updated our settlement data\n",
    "    \n",
    "    # headers (to announce ourselves to the API as friends) but they track the IP address anyways\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Plan-EO_Pipeline/1.1 (cwp5xyj@virginia.edu)'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Overpass API URL\n",
    "    url = 'http://overpass-api.de/api/interpreter'\n",
    "    \n",
    "    # send request\n",
    "    response = requests.get(url, params={'data': query}, headers = headers)\n",
    "    \n",
    "    # check for errors\n",
    "    if response.status_code != 200:\n",
    "        # if we timeout, raise a custom error (see loop for how this is handled)\n",
    "        raise APIRateLimitError(f'Error {response.status_code}: {response.text}') \n",
    "        \n",
    "    # put our new data in json format, then use convert_to_csv() to turn it into a dataframe\n",
    "    data_to_update = response.json() \n",
    "    \n",
    "    new_df = convert_to_csv(data_to_update)\n",
    "    \n",
    "    # if the convert_to_csv returns None (a.k.a., no new nodes to update) raise an exception and exit the function\n",
    "    if new_df is None:\n",
    "        raise Exception(f'No updates needed to {csv_path}')\n",
    "    \n",
    "    # call in path, checking to see that it exists (if not, need to run initialize script)\n",
    "        \n",
    "    if os.path.exists(csv_path):\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        print(f'{country_code} seems to be missing a CSV file in our system. Please initialize a fresh CSV before running the update code.')\n",
    "        return None\n",
    "\n",
    "    # concatenate new CSV, updating every line that has been changed since the last update\n",
    "    updated_df = pd.concat([existing_df[~existing_df['osm_id'].isin(new_df['osm_id'])], new_df], ignore_index=True) # bit slow but we will see\n",
    "\n",
    "    # Save back to CSV\n",
    "    updated_df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f'CSV updated successfully: {csv_path}, rows updated = {len(new_df)}') # statement to show success\n",
    "    return updated_df\n",
    "\n",
    "def make_shapefile(iso3, country, new_df):\n",
    "    \n",
    "    # this is very simple: convert updated dataframe into a spatial dataframe, then convert SDF to shapefile\n",
    "    \n",
    "    # get rid of all settlements without lat/lon data (no way to convert)\n",
    "    to_convert = new_df.dropna(axis=0, subset = ['lat', 'lon'])\n",
    "    \n",
    "    # determine geometry and create geopandas SDF using lat and lon\n",
    "    geometry = [Point(xy) for xy in zip(to_convert['lon'], to_convert['lat'])]\n",
    "    gdf = geopandas.GeoDataFrame(to_convert, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # create shapefile path, then write new SDF to it (will automatically overwrite)\n",
    "    shp_path = f'output_dir_test/{iso3}_{country}/02_Settlement_data/{iso3}_populated_places_points.shp'\n",
    "    gdf.to_file(shp_path, driver='ESRI Shapefile')\n",
    "    print(f'Shapefile updated successfully: {shp_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc6dcd6-5e38-4bd9-809d-bd8e64e769f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV updated successfully: output_dir_test/AFG_Afghanistan/02_Settlement_data/AFG_populated_places_points.csv, rows updated = 4\n",
      "Shapefile updated successfully: output_dir_test/AFG_Afghanistan/02_Settlement_data/AFG_populated_places_points.shp\n",
      "CSV updated successfully: output_dir_test/DZA_Algeria/02_Settlement_data/DZA_populated_places_points.csv, rows updated = 25\n",
      "Shapefile updated successfully: output_dir_test/DZA_Algeria/02_Settlement_data/DZA_populated_places_points.shp\n",
      "CSV updated successfully: output_dir_test/AGO_Angola/02_Settlement_data/AGO_populated_places_points.csv, rows updated = 243\n",
      "Shapefile updated successfully: output_dir_test/AGO_Angola/02_Settlement_data/AGO_populated_places_points.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_682249/618977441.py:80: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  existing_df = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV updated successfully: output_dir_test/ARG_Argentina/02_Settlement_data/ARG_populated_places_points.csv, rows updated = 70\n",
      "Shapefile updated successfully: output_dir_test/ARG_Argentina/02_Settlement_data/ARG_populated_places_points.shp\n",
      "CSV updated successfully: output_dir_test/ARM_Armenia/02_Settlement_data/ARM_populated_places_points.csv, rows updated = 17\n",
      "Shapefile updated successfully: output_dir_test/ARM_Armenia/02_Settlement_data/ARM_populated_places_points.shp\n",
      "CSV updated successfully: output_dir_test/AZE_Azerbaijan/02_Settlement_data/AZE_populated_places_points.csv, rows updated = 131\n",
      "Shapefile updated successfully: output_dir_test/AZE_Azerbaijan/02_Settlement_data/AZE_populated_places_points.shp\n",
      "CSV updated successfully: output_dir_test/BGD_Bangladesh/02_Settlement_data/BGD_populated_places_points.csv, rows updated = 12\n",
      "Shapefile updated successfully: output_dir_test/BGD_Bangladesh/02_Settlement_data/BGD_populated_places_points.shp\n",
      "No updates needed to output_dir_test/BLZ_Belize/02_Settlement_data/BLZ_populated_places_points.csv\n",
      "CSV updated successfully: output_dir_test/BEN_Benin/02_Settlement_data/BEN_populated_places_points.csv, rows updated = 255\n",
      "Shapefile updated successfully: output_dir_test/BEN_Benin/02_Settlement_data/BEN_populated_places_points.shp\n",
      "CSV updated successfully: output_dir_test/BTN_Bhutan/02_Settlement_data/BTN_populated_places_points.csv, rows updated = 1\n",
      "Shapefile updated successfully: output_dir_test/BTN_Bhutan/02_Settlement_data/BTN_populated_places_points.shp\n",
      "CPU times: user 2.31 s, sys: 409 ms, total: 2.72 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# run the below loop to update all settlement data.\n",
    "\n",
    "# for i in range(10): \n",
    "for i in range(len(metadata)):   \n",
    "    \n",
    "    # extract necessary metadata - index is clean, so \"i\" works fine\n",
    "    iso2 = metadata.loc[i, 'ISO2']\n",
    "    iso3 = metadata.loc[i,'ISO3']\n",
    "    country = metadata.loc[i,'Name']\n",
    "\n",
    "    # create path\n",
    "    path = f'../Individual_country_data/{iso3}_{country}/02_Settlement_data/{iso3}_populated_places_points.csv'\n",
    "    \n",
    "    try:\n",
    "        # this is what should happen if everything works perfectly\n",
    "        # first, update_settlements() creates and saves an updated dataframe\n",
    "        new_df = update_settlements(iso2, path)\n",
    "        # then, make_shapefile() saves the updated shapefile\n",
    "        make_shapefile(iso3, country, new_df)\n",
    "        \n",
    "    # if an exception is raised because there are no settlements to update for this country:\n",
    "    except Exception as e:\n",
    "        print(e) # print the message so the user knows, then move on to the next country\n",
    "        pass\n",
    "    \n",
    "    # if an exception is raised because of rate limiting:\n",
    "    except APIRateLimitError as e:\n",
    "        # explain it to the user\n",
    "        print('Initiating 5 minute buffer time to prevent rate limiting... updates will resume shortly.')\n",
    "        time.sleep(300) # 5 minute buffer\n",
    "        \n",
    "        # then try it all again\n",
    "        new_df = update_settlements(iso2, path)\n",
    "        make_shapefile(iso3, country, new_df)\n",
    "    \n",
    "    # should probably introduce a sleep() function here because this will still be expensive\n",
    "    # update: found that this is not necessary, but if we find that this is often breaking, uncomment and edit the below line\n",
    "    # time.sleep(5)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
